# Symbiotic Programming Index (SPI)

## Disclaimer & Positioning

This document is an **open, early-stage exploration** of the *Symbiotic Programming Index (SPI)*.  
It does **not** claim to be a final standard, but rather a **working draft** intended to spark discussion, invite critique, and evolve through community feedback.

- **Work-in-Progress**: The framework, metrics, and methodology described here are provisional and subject to revision.  
- **Community-Oriented**: The goal is to encourage collaboration across academia and industry, not to assert ownership or exclusivity.  
- **Limitations**: At this stage, validation experiments are preliminary. Reproducibility and peer-reviewed publication remain ongoing priorities.  
- **Next Steps**: Formal evaluation, dataset releases, and submissions to archival venues (e.g., ICSE, ASE, FSE, NeurIPS workshops) are planned to ensure academic rigor.

By sharing this draft publicly, we hope to **surface blind spots, refine methods, and build a shared foundation** for studying humanâ€“AI co-production in software engineering.  
Contributions, critiques, and replications are warmly welcome.

---

## ğŸ‘‰ Why SPI is Inevitable

The Symbiotic Programming Index (SPI) is not about inflating the value of a research idea. Its necessity comes from objective trends that the industry cannot avoid:

1. **Inevitable Competition of Claims**
   In five years, every vendor will say: *â€œMy AI coding workflow is the best, my IDE is the best, my coding agent is the best.â€*
   Without a reproducible yardstick, these claims will remain marketing noise. SPI exists to turn â€œbestâ€ into something measurable.

2. **Educational and Professional Demand**
   Future curricula cannot stop at teaching syntax or casual prompt tricks.
   Students will need to learn orchestration, reproducibility, and explainability as core competencies. SPI provides the metrics that define what â€œcompetenceâ€ means in an AIâ€“human workflow.

3. **Governance and Trust in Critical Domains**
   In healthcare, aviation, or finance, no one will trust a workflow just because a vendor says so.
   Certification will be required â€” just like ISO standards today. SPI is positioned as the certification framework for AI coding workflows: quality (Qc), human-off ratio (HoR), explainability (Exp), and stability (Stb).

**In short:**
SPI is not optional. It is the measuring instrument that industry, education, and governance will all require once AI coding moves from experimentation to infrastructure.

---

## ğŸŒ Background & Motivation

Conventional benchmarks such as **CodeXGLUE**, **HumanEval**, and **MBPP** measure the *capabilities of models in isolation*.  
They ask: *Can a model generate correct solutions for given tasks?*

But this is no longer the central question.  
The real challenge is: **How do humans and AI co-produce software reliably, reproducibly, and pedagogically?**

- Traditional metrics ignore **workflow quality** (replay stability, orchestration cost, reproducibility).  
- They ignore **human-off ratio** (how far humans can â€œlet goâ€ without loss of accountability).  
- They ignore **AI explainability** (can AI articulate its reasoning as a tutor, not just a coder).  

**SPI** addresses these blind spots by shifting the unit of analysis:  
ğŸ‘‰ from *models* to *workflows*.  
ğŸ‘‰ from *outputs* to *orchestration*.  
ğŸ‘‰ from *performance* to *symbiosis*.

---

## ğŸ”¬ Methodological Frame

SPI is operationalised through **five measurable dimensions**: Qc, HoR, Exp, Stb, and NLE.  

We study them across **controlled workflows**:

- **Human-only coding** â€” traditional baseline.  
- **AI-only generation** â€” single-shot, minimal orchestration.  
- **Humanâ€“AI orchestration** â€” cached prompts, validators, multi-agent flows.  

**Metrics & Validation:**

- *Qc*: unit/integration pass rates, CI/CD deploy success, static analysis.  
- *HoR*: number of edits, prompt ratio, human LOC %.  
- *Exp*: reviewer-rated clarity, learning outcomes in student studies.  
- *Stb*: testâ€“retest reproducibility, regression recurrence.  
- *NLE*: cross-lingual robustness, semantic drift under translation.  

Validation strategies include:  
- **Convergent validity** â€” SPI vs. defect density / productivity.  
- **Testâ€“retest reliability** â€” same orchestration across model versions.  
- **Cross-model generalisation** â€” GPT, Claude, DeepSeek, Gemini.  

---

## ğŸ¯ Contribution Highlights

- **Academic** â€” A reproducible taxonomy of orchestration workflows and failure modes.  
- **Industrial** â€” Criteria for hybrid QA pipelines, measuring when AI coders can be trusted.  
- **Educational** â€” Pedagogical modules where AI acts as tutor; orchestration as a core competency.  
- **Philosophical** â€” Embedding debates on authorship, agency, and epistemology into measurable indices.  

---


## ğŸ“Š Priority and Progression

The five SPI dimensions do not carry equal weight at all times.  
They form a **research staircase** â€” from practical foundations to higher-order challenges:

1. **Qc (Code Quality)** â†’ the entry point. Without quality, nothing else matters.  
2. **HoR (Human-off Ratio)** â†’ once quality is reliable, measure how far humans can step back.  
3. **Exp (Explainability)** â†’ with reduced human intervention, demand that AI explains itself intelligibly.  
4. **Stb (Stability)** â†’ the maturity test: workflows must remain reproducible across runs, contexts, and model versions.  
5. **NLE (Natural Language Engagement)** â†’ the societal horizon: true symbiosis must work across languages and cultures.

**Grouping:**
- **Qc + Stb** â†’ the **industrial foundation**, ensuring reliability and trustworthiness.  
- **HoR + Exp** â†’ the **academic and philosophical breakthrough**, redefining the role of engineers and AIâ€™s epistemic value.  
- **NLE** â†’ the **societal horizon**, preventing an English-only bottleneck and enabling global inclusivity.  

This priority ordering also mirrors **analysis difficulty and depth**:  
from **Qc (low difficulty, mid depth)** â†’ **HoR (moderate)** â†’ **Exp (high)** â†’ **Stb (very high)** â†’ **NLE (extreme)**.  
The first three are â€œvisible mountains,â€ while Stb and NLE remain â€œpeaks in the clouds.â€

---

## ğŸ“ SPI Formula

$$
\text{SPI}=\; w_{Qc}\,Qc + w_{HoR}\,HoR + w_{Exp}\,Exp + w_{Stb}\,Stb + w_{NLE}\,NLE,\quad
\text{s.t. } \sum_i w_i = 1,\; w_i \ge 0
$$

Weights $w_i$ adapt to context:
  - Industry â†’ emphasise $Qc$ + $Stb$
  - Education â†’ emphasise $HoR$ + $Exp$
  - Global fairness â†’ emphasise $NLE$


---

## ğŸ“š Documentation Structure

- **Dimensions**
  - [Code Quality (Qc)](./qc/index.md)  
  - [Human-off Ratio (HoR)](./hor/index.md)  
  - [AI Explainability (Exp)](./exp/index.md)  
  - [Stability (Stb)](./stb/index.md)  
  - [Natural Language Engagement (NLE)](./nle/index.md)  

- **Licenses**
  - [License](./license.md) â€” sharing and citation rules

---

## ğŸš€ Enduring Aim

SPI is not just about *faster coding*.  
It is about **redefining the act of programming itself**:  

- From authorship to orchestration.  
- From isolated benchmarks to reproducible indices.  
- From intuition to science.  

Our aim is to establish a **visible, teachable, and reproducible foundation** for software engineering in the AI era.  
